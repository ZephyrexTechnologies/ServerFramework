name: Compare Pytest Results

on:
  workflow_call:

jobs:
  test-pr-branch:
    runs-on: ubuntu-latest
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      has_errors: ${{ steps.extract-results.outputs.has_errors }}
      error_message: ${{ steps.extract-results.outputs.error_message }}

    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v4.2.2

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-json-report pytest-asyncio
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Discover test files
        id: discover-tests
        run: |
          echo "Looking for test files in repository..."
          TEST_FILES=$(find . -name "test_*.py" -o -name "*_test.py" | sort)
          TEST_COUNT=$(echo "$TEST_FILES" | wc -l)

          if [ "$TEST_COUNT" -eq 0 ]; then
            echo "::warning::No test files found with standard pytest naming patterns!"
            echo "Looking for alternative test file patterns..."
            ALT_TEST_FILES=$(find . -type f -name "*.py" | xargs grep -l "def test_" | sort)
            ALT_TEST_COUNT=$(echo "$ALT_TEST_FILES" | wc -l)
            
            if [ "$ALT_TEST_COUNT" -eq 0 ]; then
              echo "::error::No Python test files found in repository!"
            else
              echo "Found $ALT_TEST_COUNT Python files containing test functions:"
              echo "$ALT_TEST_FILES"
            fi
          else
            echo "Found $TEST_COUNT pytest files:"
            echo "$TEST_FILES"
          fi

      - name: Show pytest collection
        run: |
          echo "Displaying test collection (without running tests):"
          python -m pytest --collect-only -v || true

      - name: Run tests on PR Branch
        id: run-tests
        run: |
          echo "Running pytest with verbose output first to diagnose any issues:"
          python -m pytest -v --no-header --no-summary || true

          echo "Now running with JSON report for results processing:"
          # Save both stdout and stderr to files
          python -m pytest -q --json-report --json-report-file=pr_results.json > pr_stdout.txt 2> pr_stderr.txt || true

          # Debug output
          echo "Test run complete. Stdout:"
          cat pr_stdout.txt
          echo "Stderr:"
          cat pr_stderr.txt

          # Check for collection errors
          if grep -q "Interrupted: .* error during collection" pr_stderr.txt; then
            echo "Collection errors found in PR branch!"
            echo "has_collection_errors=true" >> $GITHUB_OUTPUT
            # Save error details for later
            cat pr_stderr.txt > pr_errors.txt
          else
            echo "No collection errors found in PR branch"
            echo "has_collection_errors=false" >> $GITHUB_OUTPUT
          fi

          # Check if JSON report was created
          if [ ! -f pr_results.json ]; then
            echo "::error::No test results JSON file was created!"
            echo '{"summary": {"total": 0, "passed": 0}}' > pr_results.json
            echo "has_collection_errors=true" >> $GITHUB_OUTPUT
            echo "No tests were executed" > pr_errors.txt
          fi

      - name: Extract test results
        id: extract-results
        run: |
          echo "PR_BRANCH=$(git rev-parse --abbrev-ref HEAD)" >> $GITHUB_ENV
          python -c "
          import json
          import sys
          import os
          import re

          # Check for collection errors from the previous step
          has_errors = '${{ steps.run-tests.outputs.has_collection_errors }}' == 'true'
          error_message = ''

          if has_errors:
              # Read error details if available
              if os.path.exists('pr_errors.txt'):
                  with open('pr_errors.txt', 'r') as f:
                      error_text = f.read()
                      # Extract the error summary line
                      error_match = re.search(r'(Interrupted: .* error during collection)', error_text)
                      if error_match:
                          error_message = error_match.group(1)
                      else:
                          error_message = 'Collection errors found in tests'

          # Parse test results
          with open('pr_results.json') as f:
              pr_results = json.load(f)

          # Handle missing keys with default values
          summary = pr_results.get('summary', {})
          pr_total = summary.get('total', 0)
          pr_passed = summary.get('passed', 0)
          pr_percentage = (pr_passed / pr_total * 100) if pr_total > 0 else 0

          print(f'Total tests: {pr_total}')
          print(f'Passed tests: {pr_passed}')
          print(f'Pass percentage: {pr_percentage:.2f}%')

          if has_errors:
              print(f'Collection errors found: {error_message}')

          # Set outputs for GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={pr_total}\\n')
              f.write(f'passed={pr_passed}\\n')
              f.write(f'percentage={pr_percentage:.2f}\\n')
              f.write(f'has_errors={str(has_errors).lower()}\\n')
              f.write(f'error_message={error_message}\\n')
          "

  test-main-branch:
    runs-on: ubuntu-latest
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      has_errors: ${{ steps.extract-results.outputs.has_errors }}
      error_message: ${{ steps.extract-results.outputs.error_message }}

    steps:
      - name: Checkout main branch
        uses: actions/checkout@v4.2.2
        with:
          ref: main

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-json-report pytest-asyncio
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Discover test files
        id: discover-tests
        run: |
          echo "Looking for test files in repository..."
          TEST_FILES=$(find . -name "test_*.py" -o -name "*_test.py" | sort)
          TEST_COUNT=$(echo "$TEST_FILES" | wc -l)

          if [ "$TEST_COUNT" -eq 0 ]; then
            echo "::warning::No test files found with standard pytest naming patterns!"
            echo "Looking for alternative test file patterns..."
            ALT_TEST_FILES=$(find . -type f -name "*.py" | xargs grep -l "def test_" | sort)
            ALT_TEST_COUNT=$(echo "$ALT_TEST_FILES" | wc -l)
            
            if [ "$ALT_TEST_COUNT" -eq 0 ]; then
              echo "::error::No Python test files found in repository!"
            else
              echo "Found $ALT_TEST_COUNT Python files containing test functions:"
              echo "$ALT_TEST_FILES"
            fi
          else
            echo "Found $TEST_COUNT pytest files:"
            echo "$TEST_FILES"
          fi

      - name: Show pytest collection
        run: |
          echo "Displaying test collection (without running tests):"
          python -m pytest --collect-only -v || true

      - name: Run tests on main branch
        id: run-tests
        run: |
          echo "Running pytest with verbose output first to diagnose any issues:"
          python -m pytest -v --no-header --no-summary || true

          echo "Now running with JSON report for results processing:"
          # Save both stdout and stderr to files
          python -m pytest -q --json-report --json-report-file=main_results.json > main_stdout.txt 2> main_stderr.txt || true

          # Debug output
          echo "Test run complete. Stdout:"
          cat main_stdout.txt
          echo "Stderr:"
          cat main_stderr.txt

          # Check for collection errors
          if grep -q "Interrupted: .* error during collection" main_stderr.txt; then
            echo "Collection errors found in main branch!"
            echo "has_collection_errors=true" >> $GITHUB_OUTPUT
            # Save error details for later
            cat main_stderr.txt > main_errors.txt
          else
            echo "No collection errors found in main branch"
            echo "has_collection_errors=false" >> $GITHUB_OUTPUT
          fi

          # Check if JSON report was created
          if [ ! -f main_results.json ]; then
            echo "::error::No test results JSON file was created!"
            echo '{"summary": {"total": 0, "passed": 0}}' > main_results.json
            echo "has_collection_errors=true" >> $GITHUB_OUTPUT
            echo "No tests were executed" > main_errors.txt
          fi

      - name: Extract test results
        id: extract-results
        run: |
          python -c "
          import json
          import sys
          import os
          import re

          # Check for collection errors from the previous step
          has_errors = '${{ steps.run-tests.outputs.has_collection_errors }}' == 'true'
          error_message = ''

          if has_errors:
              # Read error details if available
              if os.path.exists('main_errors.txt'):
                  with open('main_errors.txt', 'r') as f:
                      error_text = f.read()
                      # Extract the error summary line
                      error_match = re.search(r'(Interrupted: .* error during collection)', error_text)
                      if error_match:
                          error_message = error_match.group(1)
                      else:
                          error_message = 'Collection errors found in tests'

          # Parse test results
          with open('main_results.json') as f:
              main_results = json.load(f)

          # Handle missing keys with default values
          summary = main_results.get('summary', {})
          main_total = summary.get('total', 0)
          main_passed = summary.get('passed', 0)
          main_percentage = (main_passed / main_total * 100) if main_total > 0 else 0

          print(f'Total tests: {main_total}')
          print(f'Passed tests: {main_passed}')
          print(f'Pass percentage: {main_percentage:.2f}%')

          if has_errors:
              print(f'Collection errors found: {error_message}')

          # Set outputs for GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={main_total}\\n')
              f.write(f'passed={main_passed}\\n')
              f.write(f'percentage={main_percentage:.2f}\\n')
              f.write(f'has_errors={str(has_errors).lower()}\\n')
              f.write(f'error_message={error_message}\\n')
          "

  compare-results:
    needs: [test-pr-branch, test-main-branch]
    runs-on: ubuntu-latest

    steps:
      - name: Compare test results
        run: |
          echo "Main branch: ${{ needs.test-main-branch.outputs.passed }}/${{ needs.test-main-branch.outputs.total }} tests passed (${{ needs.test-main-branch.outputs.percentage }}%)"
          echo "PR branch: ${{ needs.test-pr-branch.outputs.passed }}/${{ needs.test-pr-branch.outputs.total }} tests passed (${{ needs.test-pr-branch.outputs.percentage }}%)"

          # Check for collection errors
          PR_HAS_ERRORS="${{ needs.test-pr-branch.outputs.has_errors }}"
          MAIN_HAS_ERRORS="${{ needs.test-main-branch.outputs.has_errors }}"

          if [ "$PR_HAS_ERRORS" == "true" ]; then
            echo "⚠️ PR branch has collection errors: ${{ needs.test-pr-branch.outputs.error_message }}"
          fi

          if [ "$MAIN_HAS_ERRORS" == "true" ]; then
            echo "ℹ️ Main branch has collection errors: ${{ needs.test-main-branch.outputs.error_message }}"
          fi

          # Use Python for more complex comparison logic
          python -c "
          import sys

          # Test counts
          pr_total = float('${{ needs.test-pr-branch.outputs.total }}')
          main_total = float('${{ needs.test-main-branch.outputs.total }}')
          pr_passed = float('${{ needs.test-pr-branch.outputs.passed }}')
          main_passed = float('${{ needs.test-main-branch.outputs.passed }}')
          pr_percentage = float('${{ needs.test-pr-branch.outputs.percentage }}')
          main_percentage = float('${{ needs.test-main-branch.outputs.percentage }}')

          # Error states
          pr_has_errors = '${{ needs.test-pr-branch.outputs.has_errors }}' == 'true'
          main_has_errors = '${{ needs.test-main-branch.outputs.has_errors }}' == 'true'

          # First check for collection errors
          if pr_has_errors and not main_has_errors:
              print('❌ PR branch has collection errors but main branch does not!')
              print('   This is a regression. Please fix the collection errors in your branch.')
              sys.exit(1)

          # Check for no tests
          if pr_total == 0:
              if main_total > 0:
                  print('❌ PR branch has no tests but main branch has {main_total} tests')
                  print('   This is a regression. No tests were collected in your branch.')
                  sys.exit(1)
              else:
                  print('⚠️ Neither branch has tests.')
                  if pr_has_errors or main_has_errors:
                      print('   Both branches have collection errors that need to be addressed.')
                  sys.exit(0)

          # Compare test results only if both branches have tests with no collection errors
          # or if both have collection errors (existing condition)
          if (not pr_has_errors and not main_has_errors) or (pr_has_errors and main_has_errors):
              if pr_passed >= main_passed and pr_percentage >= main_percentage:
                  print('✅ PR branch has equal or better test results than main branch')
                  if pr_has_errors and main_has_errors:
                      print('   Note: Both branches have collection errors that should be fixed.')
                  sys.exit(0)
              else:
                  print('❌ PR branch has worse test results than main branch')
                  print(f'  - Passed tests: {pr_passed} vs {main_passed} on main')
                  print(f'  - Pass percentage: {pr_percentage}% vs {main_percentage}% on main')
                  sys.exit(1)

          # If we get here, it's because the PR has fewer collection errors than main
          # which is technically an improvement
          if not pr_has_errors and main_has_errors:
              print('✅ PR branch IMPROVES on main branch by fixing collection errors!')
              sys.exit(0)

          # Fallback (shouldn't reach here with our logic)
          print('⚠️ Unexpected comparison state. Allowing tests to pass.')
          sys.exit(0)
          "
