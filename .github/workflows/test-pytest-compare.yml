name: Run and Compare Python Tests

on:
  pull_request:
    branches: [main]
  workflow_call:

jobs:
  test-pr-branch:
    runs-on: ubuntu-latest
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_errors }}

    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v4.2.2

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-json-report pytest-asyncio
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Check for test collection errors
        id: check-collection
        run: |
          # First run pytest in collection only mode
          python -m pytest --collect-only -v > collection_output.txt 2>&1 || true

          # Check for various types of collection errors
          if grep -q "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt; then
            echo "Collection errors detected:"
            grep -A 10 -B 1 "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt
            
            # Find the specific file with the error
            ERROR_FILE=$(grep -o "ERROR collecting.*\.py" collection_output.txt | grep -o "[a-zA-Z0-9_/]*\.py" || echo "Unknown file")
            
            if [[ "$ERROR_FILE" != "Unknown file" ]]; then
              echo "Error in file $ERROR_FILE"
              echo "Error details:"
              grep -A 10 "$ERROR_FILE" collection_output.txt
            fi
            
            echo "has_errors=true" >> $GITHUB_OUTPUT
          else
            echo "No collection errors detected"
            
            # Check if any tests were collected
            TEST_COUNT=$(grep -o "collected [0-9]* item" collection_output.txt | grep -o "[0-9]*" || echo "0")
            if [[ "$TEST_COUNT" == "0" ]]; then
              echo "::warning::No tests were found in the PR branch"
              echo "has_errors=true" >> $GITHUB_OUTPUT
            else  
              echo "Found $TEST_COUNT tests"
              echo "has_errors=false" >> $GITHUB_OUTPUT
            fi
          fi

          # Save the collection output for debugging
          cat collection_output.txt

      - name: Run tests on PR Branch
        run: |
          python -m pytest -v --json-report --json-report-file=pr_results.json || true

      - name: Extract test results
        id: extract-results
        run: |
          echo "PR_BRANCH=$(git rev-parse --abbrev-ref HEAD)" >> $GITHUB_ENV
          python -c "
          import json
          import sys
          import os

          # Default values in case file doesn't exist or is invalid
          pr_total = 0
          pr_passed = 0
          pr_percentage = 0

          try:
              with open('pr_results.json') as f:
                  pr_results = json.load(f)
              
              pr_total = pr_results['summary']['total']
              pr_passed = pr_results['summary']['passed']
              pr_percentage = (pr_passed / pr_total * 100) if pr_total > 0 else 0
          except Exception as e:
              print(f'Error processing results: {e}')

          print(f'Total tests: {pr_total}')
          print(f'Passed tests: {pr_passed}')
          print(f'Pass percentage: {pr_percentage:.2f}%')

          # Set outputs for GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={pr_total}\\n')
              f.write(f'passed={pr_passed}\\n')
              f.write(f'percentage={pr_percentage:.2f}\\n')
          "

  test-main-branch:
    runs-on: ubuntu-latest
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_errors }}

    steps:
      - name: Checkout main branch
        uses: actions/checkout@v4.2.2
        with:
          ref: main

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-json-report pytest-asyncio
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Check for test collection errors
        id: check-collection
        run: |
          # First run pytest in collection only mode
          python -m pytest --collect-only -v > collection_output.txt 2>&1 || true

          # Check for various types of collection errors
          if grep -q "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt; then
            echo "Collection errors detected:"
            grep -A 10 -B 1 "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt
            
            # Find the specific file with the error
            ERROR_FILE=$(grep -o "ERROR collecting.*\.py" collection_output.txt | grep -o "[a-zA-Z0-9_/]*\.py" || echo "Unknown file")
            
            if [[ "$ERROR_FILE" != "Unknown file" ]]; then
              echo "Error in file $ERROR_FILE"
              echo "Error details:"
              grep -A 10 "$ERROR_FILE" collection_output.txt
            fi
            
            echo "has_errors=true" >> $GITHUB_OUTPUT
          else
            echo "No collection errors detected"
            
            # Check if any tests were collected
            TEST_COUNT=$(grep -o "collected [0-9]* item" collection_output.txt | grep -o "[0-9]*" || echo "0")
            if [[ "$TEST_COUNT" == "0" ]]; then
              echo "::warning::No tests were found in the main branch"
              echo "has_errors=true" >> $GITHUB_OUTPUT
            else  
              echo "Found $TEST_COUNT tests"
              echo "has_errors=false" >> $GITHUB_OUTPUT
            fi
          fi

          # Save the collection output for debugging
          cat collection_output.txt

      - name: Run tests on main branch
        run: |
          python -m pytest -v --json-report --json-report-file=main_results.json || true

      - name: Extract test results
        id: extract-results
        run: |
          python -c "
          import json
          import sys
          import os

          # Default values in case file doesn't exist or is invalid
          main_total = 0
          main_passed = 0
          main_percentage = 0

          try:
              with open('main_results.json') as f:
                  main_results = json.load(f)
              
              main_total = main_results['summary']['total']
              main_passed = main_results['summary']['passed']
              main_percentage = (main_passed / main_total * 100) if main_total > 0 else 0
          except Exception as e:
              print(f'Error processing results: {e}')

          print(f'Total tests: {main_total}')
          print(f'Passed tests: {main_passed}')
          print(f'Pass percentage: {main_percentage:.2f}%')

          # Set outputs for GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={main_total}\\n')
              f.write(f'passed={main_passed}\\n')
              f.write(f'percentage={main_percentage:.2f}\\n')
          "

  compare-results:
    needs: [test-pr-branch, test-main-branch]
    runs-on: ubuntu-latest

    steps:
      - name: Install bc
        run: sudo apt-get install -y bc

      - name: Check for collection errors
        run: |
          PR_ERRORS="${{ needs.test-pr-branch.outputs.collection_errors }}"
          MAIN_ERRORS="${{ needs.test-main-branch.outputs.collection_errors }}"

          if [[ "$PR_ERRORS" == "true" ]]; then
            echo "::error::Test collection errors or no tests detected in PR branch"
            echo "❌ PR branch has test collection errors or no tests"
            exit 1
          fi

          if [[ "$MAIN_ERRORS" == "true" ]]; then
            echo "⚠️ Main branch has test collection errors or no tests"
            # Don't fail the workflow, just warn that main branch has issues
          fi

      - name: Compare test results
        run: |
          echo "Main branch: ${{ needs.test-main-branch.outputs.passed }}/${{ needs.test-main-branch.outputs.total }} tests passed (${{ needs.test-main-branch.outputs.percentage }}%)"
          echo "PR branch: ${{ needs.test-pr-branch.outputs.passed }}/${{ needs.test-pr-branch.outputs.total }} tests passed (${{ needs.test-pr-branch.outputs.percentage }}%)"

          # Check if PR branch has any tests
          if [[ "${{ needs.test-pr-branch.outputs.total }}" == "0" ]]; then
            echo "::error::No tests were found in the PR branch"
            echo "❌ PR branch has no tests detected"
            exit 1
          fi

          # Convert string outputs to numbers for comparison
          PR_PASSED=${{ needs.test-pr-branch.outputs.passed }}
          MAIN_PASSED=${{ needs.test-main-branch.outputs.passed }}
          PR_PERCENTAGE=${{ needs.test-pr-branch.outputs.percentage }}
          MAIN_PERCENTAGE=${{ needs.test-main-branch.outputs.percentage }}

          if (( $(echo "$PR_PASSED >= $MAIN_PASSED" | bc -l) )) && (( $(echo "$PR_PERCENTAGE >= $MAIN_PERCENTAGE" | bc -l) )); then
            echo "✅ PR branch has equal or better test results than main branch"
            exit 0
          else
            echo "❌ PR branch has worse test results than main branch"
            echo "  - Passed tests: $PR_PASSED vs $MAIN_PASSED on main"
            echo "  - Pass percentage: $PR_PERCENTAGE% vs $MAIN_PERCENTAGE% on main"
            exit 1
          fi
